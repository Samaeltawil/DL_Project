{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adrie\\.conda\\envs\\DL_mp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "from custom_dataset import MemeDataset\n",
    "from vilbert_adapt import MemeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1: preprocessing and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adrie\\.conda\\envs\\DL_mp\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define transformations for image preprocessing\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizing using ImageNet statistics\n",
    "])\n",
    "\n",
    "# Define dataset\n",
    "dataset_path = ''\n",
    "image_path = os.path.join(dataset_path, 'dataset/img_resized')\n",
    "text_path = os.path.join(dataset_path, 'dataset/img_txt')\n",
    "GT_path = os.path.join(dataset_path, 'dataset/MMHS150K_GT.csv')\n",
    "dataset = MemeDataset(GT_path, image_path, transform=data_transforms)\n",
    "\n",
    "# Split dataset into training, validation, and test sets \n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = (len(dataset) - train_size) // 2\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visual inspection of step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Function to display images from the dataset\n",
    "# def show_samples(dataset, num_samples=5):\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     for i in range(num_samples):\n",
    "#         image, _, _, label = dataset[i]\n",
    "#         image = image.permute(1, 2, 0)  # Reorder dimensions for visualization (C, H, W) to (H, W, C)\n",
    "#         image = image.numpy()  # Convert torch tensor to numpy array\n",
    "#         label = \"Hateful\" if label == 1 else \"Not Hateful\"\n",
    "#         plt.subplot(1, num_samples, i+1)\n",
    "#         plt.imshow(image)\n",
    "#         plt.title(label)\n",
    "#         plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# # Show samples from training dataset\n",
    "# show_samples(train_dataset, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 2: Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "\n",
    "# # Instantiate the model\n",
    "# model = VilBertForHatefulContentDetection(num_classes=2)\n",
    "\n",
    "# # Move the model to the appropriate device\n",
    "# # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Generate dummy input tensors\n",
    "# dummy_images = torch.randn(1, 3, 224, 224).to(device)  # Assuming batch size of 1 and image size of 224x224\n",
    "# dummy_input_ids = torch.randint(0, 1000, (1, 128)).to(device)  # Assuming maximum sequence length of 128 and vocabulary size of 1000\n",
    "# dummy_attention_mask = torch.ones_like(dummy_input_ids).to(device)  # Assuming all tokens are attended to\n",
    "\n",
    "# # Pass dummy inputs through the model\n",
    "# output = model(dummy_images, dummy_input_ids, dummy_attention_mask)\n",
    "\n",
    "# # Print output shape\n",
    "# print(output.shape)\n",
    "\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 3: Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# def train_model(model, data_loader, optimizer, device, scheduler, num_epochs=4):\n",
    "#     model = model.train()\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         for batch in data_loader:\n",
    "#             batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#             outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "#             loss = nn.CrossEntropyLoss()(outputs, batch['labels'])\n",
    "\n",
    "#             loss.backward()\n",
    "#             nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             optimizer.step()\n",
    "#             scheduler.step()\n",
    "#             optimizer.zero_grad()\n",
    "#         print(f'Epoch {epoch + 1}/{num_epochs}, Loss:Â {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = MemeClassifier().to(device)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adrie\\.conda\\envs\\DL_mp\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=5, learning_rate=0.001):\n",
    "    torch.cuda.empty_cache()\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "            loss = criterion(outputs, batch['labels'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch['labels'].size(0)\n",
    "            correct += (predicted == batch['labels']).sum().item()\n",
    "            running_loss += loss.item() * batch['input_ids'].size(0)\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in train_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "                loss = criterion(outputs, batch['labels'])\n",
    "                val_loss += loss.item() * batch['input_ids'].size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += batch['labels'].size(0)\n",
    "                correct += (predicted == batch['labels']).sum().item()\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_accuracy = correct / total\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "model = MemeClassifier()\n",
    "# Example usage\n",
    "train_model(model, train_loader, val_loader, num_epochs=5, learning_rate=0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 4: Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask, labels in tqdm(dataloader, desc='Evaluation'):\n",
    "            images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader.dataset)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(f'Evaluation Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Example usage\n",
    "evaluate_model(model, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_mp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
