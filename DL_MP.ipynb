{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LGKTsh9CPERX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\adrie\\.conda\\envs\\DL_mp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "from preprocessing import create_csv_labels\n",
        "from custom_dataset import CustomDataset\n",
        "from vilbert_adapt import CustomBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "on_colab = False\n",
        "create_csv = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au9o9WX5PERZ"
      },
      "source": [
        "# step 1: preprocessing and data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LOznnQ8PERa",
        "outputId": "57b351c4-4a43-4ade-f7c8-cbec34709efb"
      },
      "outputs": [],
      "source": [
        "if on_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    dataset_path = 'drive/MyDrive/DL_project'\n",
        "else:\n",
        "    dataset_path = ''\n",
        "\n",
        "# Load dataset\n",
        "image_path = os.path.join(dataset_path, 'dataset/img_resized')\n",
        "img_text_path = os.path.join(dataset_path, 'dataset/img_txt')\n",
        "json_path = os.path.join(dataset_path, 'dataset/MMHS150K_GT.json')\n",
        "GT_path = os.path.join(dataset_path, 'dataset/MMHS150K_Custom.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create cleaned csv file\n",
        "if create_csv:\n",
        "    filename = os.path.join(dataset_path, \"dataset/MMHS1150K_Custom.csv\")\n",
        "    create_csv_labels(json_path, filename, img_text_path)\n",
        "    GT_path = filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\adrie\\.conda\\envs\\DL_mp\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define transformations for image preprocessing\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizing using ImageNet statistics\n",
        "])\n",
        "\n",
        "dataset = CustomDataset(GT_path, image_path, img_text_path, transform=data_transforms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qMI92anPERa",
        "outputId": "09b5d625-fd75-41fe-d7c6-c5738bd834bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([  101,  1026,  5310,  1028,  1026,  5310,  1028,  6616,  3256,  2317,\n",
              "         10514, 28139, 22911,  2923, 11669,  2035,  1997,  2017,  2024, 16939,\n",
              "         13044,  1026, 24471,  2140,  1028,  7592,  1010,  2317,  8986,  1012,\n",
              "          2204,  1011,  9061,  1012,  2500,  2097,  2085,  3796,  2017,  2302,\n",
              "         11973,  2000,  5547,  2115, 16476,  1012,   102,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0]),\n",
              " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " tensor([[[-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
              "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
              "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
              "          ...,\n",
              "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
              "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
              "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008]],\n",
              " \n",
              "         [[-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
              "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
              "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
              "          ...,\n",
              "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
              "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
              "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182]],\n",
              " \n",
              "         [[-1.7522, -1.7522, -1.7522,  ..., -1.7522, -1.7522, -1.7522],\n",
              "          [-1.7522, -1.7522, -1.7522,  ..., -1.7522, -1.7522, -1.7522],\n",
              "          [-1.7522, -1.7522, -1.7522,  ..., -1.7522, -1.7522, -1.7522],\n",
              "          ...,\n",
              "          [-1.7522, -1.7522, -1.7522,  ..., -1.7522, -1.7522, -1.7522],\n",
              "          [-1.7522, -1.7522, -1.7522,  ..., -1.7522, -1.7522, -1.7522],\n",
              "          [-1.7522, -1.7522, -1.7522,  ..., -1.7522, -1.7522, -1.7522]]]),\n",
              " tensor([1.]))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# visual inspection\n",
        "dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8bjhiu30PERc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define hyperparameters -------------------------------------------------------\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Split dataset into training, validation, and test sets\n",
        "dataset_size = len(dataset)\n",
        "train_set, test_set, val_set = torch.utils.data.dataset.random_split(dataset, [0.8, 0.1, 0.1])\n",
        "\n",
        "# Create data loader for training set\n",
        "not_hate_indices = []\n",
        "hate_indices = []\n",
        "for idx in range(len(train_set)):\n",
        "    if train_set[idx][2] == 1:\n",
        "        hate_indices.append(idx)\n",
        "    else:\n",
        "        not_hate_indices.append(idx)\n",
        "\n",
        "num_not_hate = len(not_hate_indices)\n",
        "num_hate = len(hate_indices)\n",
        "total_samples = num_not_hate + num_hate\n",
        "\n",
        "# Create a WeightedRandomSampler to balance the training dataset\n",
        "class_weights = [1-num_hate/total_samples, 1-num_not_hate/ total_samples]  # Inverse of number of samples per class\n",
        "\n",
        "weights = []\n",
        "for idx in range(len(train_set)):\n",
        "    try:\n",
        "        label = dataset[idx][2]\n",
        "        according_weights = class_weights[int(label)]\n",
        "        weights.append(according_weights)\n",
        "    except:\n",
        "        print(f\"Error with idx: {idx}\")\n",
        "        print(f\"Label: {dataset[idx][2]}\")\n",
        "\n",
        "# weights = [class_weights[int(dataset[idx]['label'])] for idx in train_indices]\n",
        "sampler = WeightedRandomSampler(weights, len(weights))\n",
        "\n",
        "# Create data loader for balanced training set\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, sampler=sampler)\n",
        "\n",
        "# Create data loaders for validation and test sets\n",
        "validation_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Moo9Uy85PERc"
      },
      "source": [
        "# step 2: Model building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9vwPvJ4qPERd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CustomBert(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = CustomBert()\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iwhxgMDbPERd",
        "outputId": "86fd4f13-f8d5-4a09-a705-d7fbb6a91ac7"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 54\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;66;03m# print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[9], line 27\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m     25\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m (outputs\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     26\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     29\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=1, learning_rate=0.001):\n",
        "    torch.cuda.empty_cache()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.BCELoss() # Binary cross-entropy loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i,batch in enumerate(train_loader):\n",
        "            # print(f\"Batch {i}\")\n",
        "            batch = [b.to(device) for b in batch]\n",
        "            text, mask, labels = batch\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(text, mask)\n",
        "            loss = criterion(outputs, labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            predicted = (outputs.detach() > 0.5)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = correct / total\n",
        "\n",
        "        # # Validation phase\n",
        "        # model.eval()\n",
        "        # val_loss = 0.0\n",
        "        # correct = 0\n",
        "        # total = 0\n",
        "        # with torch.no_grad():\n",
        "        #     for batch in train_loader:\n",
        "        #         batch = [b.to(device) for b in batch]\n",
        "        #         text, mask, labels = batch\n",
        "        #         outputs = model(text, mask)\n",
        "        #         loss = criterion(outputs, labels.float())\n",
        "        #         val_loss += loss.item() * text.size(0)\n",
        "        #         _, predicted = torch.max(outputs.data, 1)\n",
        "        #         total += labels.size(0)\n",
        "        #         correct += (predicted == labels.sum().item())\n",
        "        # val_loss = val_loss / len(val_loader.dataset)\n",
        "        # val_accuracy = correct / total\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
        "        # print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "# Example usage\n",
        "train_model(model, train_loader, validation_loader, num_epochs=5, learning_rate=0.0001)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nORolmKhPERe"
      },
      "source": [
        "# step 4: Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeTuOaiMPERf"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# def evaluate_model(model, dataloader):\n",
        "#     # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model.eval()\n",
        "\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#     total_loss = 0.0\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for images, input_ids, attention_mask, labels in tqdm(dataloader, desc='Evaluation'):\n",
        "#             images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "#             outputs = model(images, input_ids, attention_mask)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             total_loss += loss.item() * images.size(0)\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "#             total += labels.size(0)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "\n",
        "#     average_loss = total_loss / len(dataloader.dataset)\n",
        "#     accuracy = correct / total\n",
        "\n",
        "#     print(f'Evaluation Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# # Example usage\n",
        "# evaluate_model(model, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UE2cKjZ_PERf"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMYi8DAbPERf"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
