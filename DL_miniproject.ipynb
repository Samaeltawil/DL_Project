{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r3QtameiMgUU"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.io import read_image, read_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9ZQfvAtNxEl",
        "outputId": "f112d1cf-ed6d-4b9e-dbf6-02b4c01b24c5"
      },
      "outputs": [],
      "source": [
        "dataset_path = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8j0yrDyNVbB",
        "outputId": "c389f651-3162-4c7a-9458-1757c9c8e544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File count: 150000\n"
          ]
        }
      ],
      "source": [
        "# check to see if we have good number of pictures\n",
        "dir_path = r'dataset/img_resized'\n",
        "# print(os.listdir(dir_path))\n",
        "count = 0\n",
        "for item in os.listdir(dir_path):\n",
        "    # check if current path is a file\n",
        "    count += 1\n",
        "print('File count:', count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1DBzXEVQycB",
        "outputId": "5a446ec8-273f-4c6b-fbac-20faded1b6bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded anns: 149823\n",
            "Total Tweets Majority Voting: Not Hate: 112845, Hate: 36978, Racist: 11925, Sexist: 3495, Homophobe: 3870, Religion: 163, Other: 5811\n"
          ]
        }
      ],
      "source": [
        "anns = json.load(open(\"dataset/MMHS150K_GT.json\",\"r\"))\n",
        "print(\"Loaded anns: \" + str(len(anns)))\n",
        "\n",
        "majority_not_hate = 0\n",
        "majority_hate = 0\n",
        "majority_racist = 0\n",
        "majority_sexist = 0\n",
        "majority_homo = 0\n",
        "majority_religion = 0\n",
        "majority_other = 0\n",
        "\n",
        "for k,v in anns.items():\n",
        "    labels = []\n",
        "    label_num = []\n",
        "    # print(len(v[\"labels_str\"]))\n",
        "    for label in v[\"labels_str\"]:\n",
        "        if \"Not\" in label:\n",
        "            label_num.append(0)\n",
        "        elif \"Racist\" in label:\n",
        "            label_num.append(1)\n",
        "        elif \"Sexist\" in label:\n",
        "            label_num.append(2)\n",
        "        elif \"Homo\" in label:\n",
        "            label_num.append(3)\n",
        "        elif \"Religion\" in label:\n",
        "            label_num.append(4)\n",
        "        elif \"Other\" in label:\n",
        "            label_num.append(5)\n",
        "        else:\n",
        "            print(\"Error with: \" + label)\n",
        "            label = \"Error\"\n",
        "\n",
        "    if label_num.count(0) > 1:\n",
        "        majority_not_hate+=1\n",
        "    else:\n",
        "        majority_hate+=1\n",
        "        if label_num.count(1) > 1:\n",
        "            majority_racist+=1\n",
        "        elif label_num.count(2) > 1:\n",
        "            majority_sexist+=1\n",
        "        elif label_num.count(3) > 1:\n",
        "            majority_homo+=1\n",
        "        elif label_num.count(4) > 1:\n",
        "            majority_religion+=1\n",
        "        elif label_num.count(5) > 1:\n",
        "            majority_other+=1\n",
        "\n",
        "print(\"Total Tweets Majority Voting: Not Hate: \" + str(majority_not_hate) + \", Hate: \" + str(majority_hate) + \", Racist: \" + str(majority_racist) + \", Sexist: \" + str(majority_sexist) + \", Homophobe: \" + str(majority_homo) + \", Religion: \" + str(majority_religion) + \", Other: \" + str(majority_other))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Gd5Wdfr-Mh9_"
      },
      "outputs": [],
      "source": [
        "image_path = os.path.join(dataset_path, 'dataset/img_resized')\n",
        "text_path = os.path.join(dataset_path, 'dataset/img_txt')\n",
        "GT_path = os.path.join(dataset_path, 'dataset/MMHS150K_GT.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KIg3dJkzMmzC"
      },
      "outputs": [],
      "source": [
        "from preprocessing import create_csv_labels\n",
        "# uncomment the first time\n",
        "create_csv_labels(os.path.join(dataset_path, 'dataset/MMHS150K_GT.json'), GT_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TV_kvlJJMpdS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset/img_resized/1110145718201651200.jpg\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ID': 1110145718201651200,\n",
              " 'text': \"it's time to work out mah nigga❤️ <url>\",\n",
              " 'image': tensor([[[ 10,  10,   7,  ..., 178, 190, 189],\n",
              "          [  7,   9,   7,  ..., 163, 178, 178],\n",
              "          [  4,   7,   6,  ..., 174, 179, 181],\n",
              "          ...,\n",
              "          [ 17,  15,  14,  ..., 208, 217, 215],\n",
              "          [ 19,  18,  17,  ..., 212, 211, 209],\n",
              "          [ 24,  23,  21,  ..., 212, 203, 201]],\n",
              " \n",
              "         [[ 32,  32,  31,  ..., 156, 166, 165],\n",
              "          [ 29,  31,  31,  ..., 141, 154, 154],\n",
              "          [ 26,  29,  30,  ..., 152, 155, 157],\n",
              "          ...,\n",
              "          [ 17,  15,  14,  ..., 150, 159, 157],\n",
              "          [ 19,  18,  17,  ..., 154, 153, 151],\n",
              "          [ 24,  23,  21,  ..., 154, 145, 143]],\n",
              " \n",
              "         [[ 45,  45,  43,  ..., 145, 154, 153],\n",
              "          [ 42,  44,  43,  ..., 130, 142, 142],\n",
              "          [ 39,  42,  42,  ..., 141, 143, 145],\n",
              "          ...,\n",
              "          [ 17,  15,  14,  ...,  77,  86,  84],\n",
              "          [ 19,  18,  17,  ...,  81,  80,  78],\n",
              "          [ 24,  23,  21,  ...,  81,  72,  70]]], dtype=torch.uint8),\n",
              " 'label': 0.0}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class MMHS_150KDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, GT_path, image_path, transform=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.GT_path = GT_path\n",
        "        self.GT_data = pd.read_csv(GT_path)\n",
        "        self.idx_list = []\n",
        "        # self.root_dir = root_dir\n",
        "        self.image_path = image_path\n",
        "        # self.text_path = text_path\n",
        "        self.transform = transform\n",
        "\n",
        "        self.refine_images()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        ID = self.GT_data.iloc[idx, 0]\n",
        "        # txt_path = os.path.join(self.text_path, str(ID) + '.json').replace(\"\\\\\",\"/\")\n",
        "        img_path = os.path.join(self.image_path, str(ID) + '.jpg').replace(\"\\\\\",\"/\")\n",
        "        print(img_path)\n",
        "        # print(txt_path)\n",
        "\n",
        "        try:\n",
        "          image = read_image(img_path)\n",
        "        except:\n",
        "          pass\n",
        "        # f = open(text_path, 'r')\n",
        "        # data = json.load(f)\n",
        "        # text = data['img_text']\n",
        "        # text = f.read()\n",
        "\n",
        "        # text = read_file(txt_path)\n",
        "\n",
        "        text = self.GT_data.iloc[idx, 3]\n",
        "\n",
        "        label = self.GT_data.iloc[idx, 2]\n",
        "\n",
        "        # sample = {'ID': ID, 'text': text, 'label': label}\n",
        "        sample = {'ID': ID, 'text': text, 'image': image, 'label': label}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def refine_images(self):\n",
        "      GT_path_cleared = os.path.join(dataset_path, 'dataset/MMHS150K_GT_cleared.csv')\n",
        "      cmpt = 0\n",
        "      with open(GT_path, 'r') as readFile:\n",
        "        lines = readFile.readlines()\n",
        "        for row in lines[1:]:\n",
        "          idx = row[0:19]\n",
        "          img_path = os.path.join(self.image_path, str(idx) + '.jpg').replace(\"\\\\\",\"/\")\n",
        "          # look if the image exists, otherwise delete the idx from the csv\n",
        "          if not (os.path.isfile(img_path)):\n",
        "            lines.remove(row)\n",
        "            cmpt = cmpt + 1\n",
        "\n",
        "      with open(GT_path_cleared, 'w') as writeFile:\n",
        "        for line in lines:\n",
        "          writeFile.write(line)\n",
        "\n",
        "      self.len_samples = len(lines)\n",
        "      self.GT_data =  pd.read_csv(GT_path_cleared)\n",
        "      # print(f\"nbr_missing = {cmpt}\")\n",
        "      # print(f\"len = {len(self.GT_data.iloc[:, 0])}\")\n",
        "      # print(f\"linelen = {len(lines)}\")\n",
        "\n",
        "# example = MMHS_150KDataset(GT_path, image_path)\n",
        "# example[1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuG8UDLhb3Ro",
        "outputId": "099fa90f-295e-4952-ea38-7e2ffcbfa087"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "149824\n",
            "14982 7491\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "dataset = MMHS_150KDataset(GT_path, image_path)\n",
        "batch_size = 16\n",
        "validation_split = .1\n",
        "test_split = .5 # corresponds to half ot the validation set\n",
        "shuffle_dataset = True\n",
        "random_seed= 42\n",
        "\n",
        "# Creating data indices for training and validation splits:\n",
        "dataset_size = len(dataset)\n",
        "print(dataset_size)\n",
        "indices = list(range(dataset_size))\n",
        "validation_split = int(np.floor(validation_split * dataset_size))\n",
        "test_split = int(np.floor(test_split * validation_split))\n",
        "print(validation_split, test_split)\n",
        "if shuffle_dataset :\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices, test_indices = indices[validation_split:], indices[test_split:validation_split], indices[:test_split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "test_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                           sampler=train_sampler)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                                sampler=valid_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                                sampler=test_sampler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WbckVLd7aTux"
      },
      "outputs": [],
      "source": [
        "# class MMHS150KDataset(Dataset):\n",
        "#     \"\"\"MMHS150K dataset.\"\"\"\n",
        "\n",
        "#     def __init__(self, label_id_path, txt_path, images_path, transform=None):\n",
        "#         \"\"\"\n",
        "#         Arguments:\n",
        "#             label_id_path (string): Path to the MMHS150K_GT.json which contains\n",
        "#                                     the IDs and labels corresponding\n",
        "#             txt_path (string): Path to the text file with annotations.\n",
        "#             images_path (string): Directory with all the images.\n",
        "#             transform (callable, optional): Optional transform to be applied\n",
        "#                 on a sample.\n",
        "#         \"\"\"\n",
        "#         self.label_id_path = label_id_path\n",
        "#         self.txt_path = txt_path\n",
        "#         self.images_path = images_path\n",
        "#         self.transform = transform"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
