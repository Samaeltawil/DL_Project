{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r3QtameiMgUU"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.io import read_image, read_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9ZQfvAtNxEl",
        "outputId": "f112d1cf-ed6d-4b9e-dbf6-02b4c01b24c5"
      },
      "outputs": [],
      "source": [
        "dataset_path = '../dataset_full/' # to modify according to your folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8j0yrDyNVbB",
        "outputId": "c389f651-3162-4c7a-9458-1757c9c8e544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File count: 150000\n"
          ]
        }
      ],
      "source": [
        "# check to see if we have good number of pictures\n",
        "dir_path = r'../dataset_full/img_resized'\n",
        "# print(os.listdir(dir_path))\n",
        "count = 0\n",
        "for item in os.listdir(dir_path):\n",
        "    # check if current path is a file\n",
        "    count += 1\n",
        "print('File count:', count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1DBzXEVQycB",
        "outputId": "5a446ec8-273f-4c6b-fbac-20faded1b6bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded anns: 149823\n",
            "Total Tweets Majority Voting: Not Hate: 112845, Hate: 36978, Racist: 11925, Sexist: 3495, Homophobe: 3870, Religion: 163, Other: 5811\n"
          ]
        }
      ],
      "source": [
        "anns = json.load(open(dataset_path + 'MMHS150K_GT.json',\"r\"))\n",
        "print(\"Loaded anns: \" + str(len(anns)))\n",
        "\n",
        "majority_not_hate = 0\n",
        "majority_hate = 0\n",
        "majority_racist = 0\n",
        "majority_sexist = 0\n",
        "majority_homo = 0\n",
        "majority_religion = 0\n",
        "majority_other = 0\n",
        "\n",
        "for k,v in anns.items():\n",
        "    labels = []\n",
        "    label_num = []\n",
        "    # print(len(v[\"labels_str\"]))\n",
        "    for label in v[\"labels_str\"]:\n",
        "        if \"Not\" in label:\n",
        "            label_num.append(0)\n",
        "        elif \"Racist\" in label:\n",
        "            label_num.append(1)\n",
        "        elif \"Sexist\" in label:\n",
        "            label_num.append(2)\n",
        "        elif \"Homo\" in label:\n",
        "            label_num.append(3)\n",
        "        elif \"Religion\" in label:\n",
        "            label_num.append(4)\n",
        "        elif \"Other\" in label:\n",
        "            label_num.append(5)\n",
        "        else:\n",
        "            print(\"Error with: \" + label)\n",
        "            label = \"Error\"\n",
        "\n",
        "    if label_num.count(0) > 1:\n",
        "        majority_not_hate+=1\n",
        "    else:\n",
        "        majority_hate+=1\n",
        "        if label_num.count(1) > 1:\n",
        "            majority_racist+=1\n",
        "        elif label_num.count(2) > 1:\n",
        "            majority_sexist+=1\n",
        "        elif label_num.count(3) > 1:\n",
        "            majority_homo+=1\n",
        "        elif label_num.count(4) > 1:\n",
        "            majority_religion+=1\n",
        "        elif label_num.count(5) > 1:\n",
        "            majority_other+=1\n",
        "\n",
        "print(\"Total Tweets Majority Voting: Not Hate: \" + str(majority_not_hate) + \", Hate: \" + str(majority_hate) + \", Racist: \" + str(majority_racist) + \", Sexist: \" + str(majority_sexist) + \", Homophobe: \" + str(majority_homo) + \", Religion: \" + str(majority_religion) + \", Other: \" + str(majority_other))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Gd5Wdfr-Mh9_"
      },
      "outputs": [],
      "source": [
        "image_path = os.path.join(dataset_path, 'img_resized')\n",
        "text_path = os.path.join(dataset_path, 'img_txt')\n",
        "GT_path = os.path.join(dataset_path, 'MMHS150K_GT.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../dataset_full/MMHS150K_GT.csv\n"
          ]
        }
      ],
      "source": [
        "print(GT_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KIg3dJkzMmzC"
      },
      "outputs": [],
      "source": [
        "from preprocessing import create_csv_labels\n",
        "# uncomment the first time\n",
        "# create_csv_labels(os.path.join(dataset_path, 'MMHS150K_GT.json'), GT_path, text_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "TV_kvlJJMpdS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../dataset_full/img_resized_2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# from PIL import Image\n",
        "# output_path =  os.path.join(dataset_path, 'img_resized_2')\n",
        "# print(output_path)\n",
        "# class MMHS_150KDataset(Dataset):\n",
        "#     \"\"\"MMHS_150K dataset.\"\"\"\n",
        "\n",
        "#     def __init__(self, GT_path, image_path, Rescale=None, transform=None):\n",
        "#         \"\"\"\n",
        "#         Arguments:\n",
        "#             GT_path (string): Path to the json file with annotations.\n",
        "#             root_dir (string): Directory with all the images.\n",
        "#             transform (callable, optional): Optional transform to be applied\n",
        "#                 on a sample.\n",
        "#         \"\"\"\n",
        "#         self.GT_path = GT_path\n",
        "#         self.GT_data = pd.read_csv(GT_path)\n",
        "#         self.idx_list = []\n",
        "#         self.output_size = Rescale\n",
        "#         (self.new_width, self.new_height) = (Rescale, Rescale)\n",
        "        \n",
        "#         self.image_path = image_path\n",
        "#         self.transform = transform\n",
        "\n",
        "#         self.refine_images()\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.len_samples\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         if torch.is_tensor(idx):\n",
        "#             idx = idx.tolist()\n",
        "\n",
        "#         ID = self.GT_data.iloc[idx, 0]\n",
        "#         # txt_path = os.path.join(self.text_path, str(ID) + '.json').replace(\"\\\\\",\"/\")\n",
        "#         img_path = os.path.join(self.image_path, str(ID) + '.jpg').replace(\"\\\\\",\"/\")\n",
        "        \n",
        "#         image = read_image(img_path)\n",
        "\n",
        "#         text = self.GT_data.iloc[idx, 3]\n",
        "\n",
        "#         label = self.GT_data.iloc[idx, 2]\n",
        "\n",
        "#         sample = {'ID': ID, 'text': text, 'image': image, 'label': label}\n",
        "#         if self.transform:\n",
        "#             sample = self.transform(sample)\n",
        "\n",
        "#         return sample\n",
        "\n",
        "#     def refine_images(self):\n",
        "#       GT_path_cleared = os.path.join(dataset_path, 'MMHS150K_GT_cleared.csv')\n",
        "#       cmpt = 0\n",
        "#       with open(GT_path, 'r') as readFile:\n",
        "#         lines = readFile.readlines()\n",
        "#         for row in lines[1:]:\n",
        "#           idx = row[0:19]\n",
        "#           img_path = os.path.join(self.image_path, str(idx) + '.jpg').replace(\"\\\\\",\"/\")\n",
        "#           # look if the image exists, otherwise delete the idx from the csv\n",
        "#           if not (os.path.isfile(img_path)):\n",
        "#             lines.remove(row)\n",
        "#             cmpt = cmpt + 1\n",
        "\n",
        "#       with open(GT_path_cleared, 'w') as writeFile:\n",
        "#         for line in lines:\n",
        "#           writeFile.write(line)\n",
        "\n",
        "#       self.len_samples = len(lines)\n",
        "#       self.GT_data =  pd.read_csv(GT_path_cleared)\n",
        "#       # print(f\"nbr_missing = {cmpt}\")\n",
        "#       # print(f\"len = {len(self.GT_data.iloc[:, 0])}\")\n",
        "#       # print(f\"linelen = {len(lines)}\")\n",
        "\n",
        "# example = MMHS_150KDataset(GT_path, image_path, Rescale=299)\n",
        "# # example[100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class MMHS_150KDataset(Dataset):\n",
        "    \"\"\"MMHS_150K dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, GT_path, image_path, Rescale=None, transform=None, save_path=None, create_resized_folder=False):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            GT_path (string): Path to the json file with annotations.\n",
        "            image_path (string): Directory with all the images.\n",
        "            Rescale (int): Desired size for resizing the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "            save_path (string): Directory to save the processed images.\n",
        "        \"\"\"\n",
        "        self.GT_path = GT_path\n",
        "        self.GT_data = pd.read_csv(GT_path)\n",
        "        self.image_path = image_path\n",
        "        self.transform = transform\n",
        "\n",
        "        # Resize parameters\n",
        "        self.output_size = Rescale\n",
        "        self.resize_transform = transforms.Resize((Rescale, Rescale))\n",
        "        self.create_resized_folder = create_resized_folder\n",
        "        self.save_path = save_path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.GT_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        ID = self.GT_data.iloc[idx, 0]\n",
        "        img_path = os.path.join(self.image_path, str(ID) + '_processed.jpg').replace(\"\\\\\",\"/\")\n",
        "\n",
        "        ############################################\n",
        "        if self.save_path:\n",
        "            self.create_resized_images()\n",
        "        ############################################\n",
        "\n",
        "        image = read_image(img_path)\n",
        "\n",
        "        text = self.GT_data.iloc[idx, 3]\n",
        "        label = self.GT_data.iloc[idx, 2]\n",
        "\n",
        "        sample = {'ID': ID, 'text': text, 'image': image, 'label': label}\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "    \n",
        "    def resize_image(self, image):\n",
        "        # Resize the image to maintain aspect ratio and fit into (500, 500)\n",
        "        width, height = image.size\n",
        "        aspect_ratio = width / height\n",
        "\n",
        "        resized_image = self.resize_transform(image)\n",
        "        \n",
        "        # Crop the image to (500, 500)\n",
        "        center_crop = transforms.CenterCrop((500, 500))\n",
        "        cropped_image = center_crop(resized_image)\n",
        "\n",
        "        return cropped_image\n",
        "    \n",
        "    def create_resized_images(self, img_path):\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Resize and crop image\n",
        "        image = self.resize_image(image)\n",
        "\n",
        "        # Save processed image\n",
        "        if not os.path.exists(self.save_path):\n",
        "            os.makedirs(self.save_path)\n",
        "        save_img_path = os.path.join(self.save_path, str(ID) + '_processed.jpg')\n",
        "        image.save(save_img_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5321\n"
          ]
        }
      ],
      "source": [
        "#########################################\n",
        "## CELL TO RUN TO SAVE IMAGES IN DESIRED FOLDER\n",
        "#########################################\n",
        "\n",
        "#specify HERE the path where you wish to save the processed images\n",
        "image_path =  os.path.join(dataset_path, 'img_resized_2')\n",
        "\n",
        "dataset = MMHS_150KDataset(GT_path, image_path, Rescale=500, save_path=True)\n",
        "for idx in range(len(dataset)):\n",
        "    sample = dataset[idx]\n",
        "\n",
        "#########################################\n",
        "#########################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuG8UDLhb3Ro",
        "outputId": "099fa90f-295e-4952-ea38-7e2ffcbfa087"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "59252\n",
            "5925 2962\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "# dataset = MMHS_150KDataset(GT_path, image_path, Rescale=299)\n",
        "img_size = 299\n",
        "batch_size = 16\n",
        "validation_split = .1\n",
        "test_split = .5 # corresponds to half ot the validation set\n",
        "shuffle_dataset = True\n",
        "random_seed= 42\n",
        "\n",
        "# Creating data indices for training and validation splits:\n",
        "dataset_size = len(dataset)\n",
        "print(dataset_size)\n",
        "indices = list(range(dataset_size))\n",
        "validation_split = int(np.floor(validation_split * dataset_size))\n",
        "test_split = int(np.floor(test_split * validation_split))\n",
        "print(validation_split, test_split)\n",
        "if shuffle_dataset :\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices, test_indices = indices[validation_split:], indices[test_split:validation_split], indices[:test_split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "test_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                           sampler=train_sampler)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                                sampler=valid_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                                sampler=test_sampler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ID': tensor([1114737531403157504, 1113601326901026818, 1058910338870140928,\n",
              "         1106671930705432577, 1109824530644721664, 1109213343225647106,\n",
              "         1117055518135267334, 1109104313962557441, 1113604477121105920,\n",
              "         1061375017919959040, 1063469975942426624, 1113390973302845445,\n",
              "         1109968509503750145, 1105241583149113345, 1107273212793184256,\n",
              "         1108022963616575489]),\n",
              " 'text': ['fuck another nigga ima fuck yo sista <url>',\n",
              "  'lmfao nigga cristian stupid <url>',\n",
              "  'look at this retarded shit i found <url>',\n",
              "  '<user> nigga ain’t even hit puberty yet <url>',\n",
              "  '<user> nigga! u know i’m always ready!  we ain’t wearin that but shit they got on tho  <repeat> <url>',\n",
              "  '<user> mileena gang nigga  what’s hood ? <repeat> <url>',\n",
              "  '<user> nigga  no  <url>',\n",
              "  \"<user> <user> <user> get lost zionazi you're blocked <url>\",\n",
              "  'why was i just in a party with a luxery nigga then this happens <url>',\n",
              "  'when your baby finally come out the womb but it looks a little retarded <url>',\n",
              "  'watch it  <url> brunette milf with big tits ava addams has her tight cunt licked out  <repeat> <url>',\n",
              "  'happy birth to the annoying  stupid bitch ass nigga bert 🤠 <url>',\n",
              "  'do i look like a nigga to play with <url>',\n",
              "  'a nigga <allcaps> was <allcaps> glowing <allcaps>  fawk <allcaps> yu <allcaps> mean <allcaps>  <url>',\n",
              "  'ghanaians please  don’t forget my nigga too oo   <repeat> vote for <user>   <repeat>💪🏿💪🏿💪🏿🔥🔥🔥 pa <elong> pa <elong>   <repeat>real g ! <hashtag>  medikal <url>',\n",
              "  'if you dont know now you know nigga😌 <url>'],\n",
              " 'image': tensor([[[[253, 253, 253,  ..., 255, 255, 255],\n",
              "           [253, 253, 253,  ..., 255, 255, 255],\n",
              "           [253, 253, 253,  ..., 255, 255, 255],\n",
              "           ...,\n",
              "           [ 26,  26,  27,  ...,  16,  16,  16],\n",
              "           [ 27,  28,  28,  ...,  16,  16,  16],\n",
              "           [ 28,  29,  29,  ...,  16,  16,  16]],\n",
              " \n",
              "          [[253, 253, 253,  ..., 255, 255, 255],\n",
              "           [253, 253, 253,  ..., 255, 255, 255],\n",
              "           [253, 253, 253,  ..., 255, 255, 255],\n",
              "           ...,\n",
              "           [ 26,  26,  27,  ...,  17,  17,  17],\n",
              "           [ 27,  28,  28,  ...,  17,  17,  17],\n",
              "           [ 28,  29,  29,  ...,  17,  17,  17]],\n",
              " \n",
              "          [[253, 253, 253,  ..., 255, 255, 255],\n",
              "           [253, 253, 253,  ..., 255, 255, 255],\n",
              "           [253, 253, 253,  ..., 255, 255, 255],\n",
              "           ...,\n",
              "           [ 24,  24,  25,  ...,  12,  12,  12],\n",
              "           [ 25,  26,  26,  ...,  12,  12,  12],\n",
              "           [ 26,  27,  27,  ...,  12,  12,  12]]],\n",
              " \n",
              " \n",
              "         [[[  0,   0,   0,  ...,   0,   0,   0],\n",
              "           [  0,   0,   0,  ...,   0,   0,   0],\n",
              "           [  0,   0,   0,  ...,   0,   0,   0],\n",
              "           ...,\n",
              "           [  0,   0,   0,  ...,   0,   0,   0],\n",
              "           [  0,   0,   0,  ...,   0,   0,   0],\n",
              "           [  0,   0,   0,  ...,   0,   0,   0]],\n",
              " \n",
              "          [[  0,   0,   0,  ...,   0,   0,   0],\n",
              "           [  0,   0,   0,  ...,   0,   0,   0],\n",
              "           [  0,   0,   0,  ...,   0,   0,   0],\n",
              "           ...,\n",
              "           [  0,   0,   0,  ...,   0,   0,   0],\n",
              "           [  0,   0,   0,  ...,   0,   0,   0],\n",
              "           [  0,   0,   0,  ...,   0,   0,   0]],\n",
              " \n",
              "          [[  0,   0,   0,  ...,   0,   0,   0],\n",
              "           [  0,   0,   0,  ...,   0,   0,   0],\n",
              "           [  0,   0,   0,  ...,   0,   0,   0],\n",
              "           ...,\n",
              "           [  0,   0,   0,  ...,   0,   0,   0],\n",
              "           [  0,   0,   0,  ...,   0,   0,   0],\n",
              "           [  0,   0,   0,  ...,   0,   0,   0]]],\n",
              " \n",
              " \n",
              "         [[[250, 250, 250,  ..., 250, 250, 250],\n",
              "           [250, 250, 250,  ..., 250, 250, 250],\n",
              "           [250, 250, 250,  ..., 250, 250, 250],\n",
              "           ...,\n",
              "           [250, 250, 250,  ..., 250, 250, 250],\n",
              "           [250, 250, 250,  ..., 250, 250, 250],\n",
              "           [250, 250, 250,  ..., 250, 250, 250]],\n",
              " \n",
              "          [[250, 250, 250,  ..., 250, 250, 250],\n",
              "           [250, 250, 250,  ..., 250, 250, 250],\n",
              "           [250, 250, 250,  ..., 250, 250, 250],\n",
              "           ...,\n",
              "           [250, 250, 250,  ..., 250, 250, 250],\n",
              "           [250, 250, 250,  ..., 250, 250, 250],\n",
              "           [250, 250, 250,  ..., 250, 250, 250]],\n",
              " \n",
              "          [[250, 250, 250,  ..., 250, 250, 250],\n",
              "           [250, 250, 250,  ..., 250, 250, 250],\n",
              "           [250, 250, 250,  ..., 250, 250, 250],\n",
              "           ...,\n",
              "           [250, 250, 250,  ..., 250, 250, 250],\n",
              "           [250, 250, 250,  ..., 250, 250, 250],\n",
              "           [250, 250, 250,  ..., 250, 250, 250]]],\n",
              " \n",
              " \n",
              "         ...,\n",
              " \n",
              " \n",
              "         [[[186, 186, 186,  ..., 153, 153, 153],\n",
              "           [186, 186, 186,  ..., 153, 153, 153],\n",
              "           [185, 185, 185,  ..., 153, 153, 153],\n",
              "           ...,\n",
              "           [134, 134, 134,  ..., 126, 127, 128],\n",
              "           [134, 134, 134,  ..., 126, 127, 128],\n",
              "           [135, 135, 135,  ..., 126, 127, 128]],\n",
              " \n",
              "          [[221, 221, 221,  ..., 101, 101, 101],\n",
              "           [221, 221, 221,  ..., 101, 101, 101],\n",
              "           [220, 220, 220,  ..., 101, 101, 101],\n",
              "           ...,\n",
              "           [123, 123, 123,  ..., 121, 122, 123],\n",
              "           [123, 123, 123,  ..., 121, 122, 123],\n",
              "           [124, 124, 124,  ..., 121, 122, 123]],\n",
              " \n",
              "          [[251, 251, 251,  ..., 167, 167, 167],\n",
              "           [251, 251, 251,  ..., 167, 167, 167],\n",
              "           [248, 248, 248,  ..., 167, 167, 167],\n",
              "           ...,\n",
              "           [121, 121, 121,  ..., 117, 118, 119],\n",
              "           [121, 121, 121,  ..., 117, 118, 119],\n",
              "           [122, 122, 122,  ..., 117, 118, 119]]],\n",
              " \n",
              " \n",
              "         [[[ 67,  67,  67,  ...,  77,  77,  77],\n",
              "           [ 67,  67,  67,  ...,  77,  77,  77],\n",
              "           [ 67,  67,  67,  ...,  75,  75,  75],\n",
              "           ...,\n",
              "           [ 69,  69,  69,  ...,  58,  64,  67],\n",
              "           [ 66,  66,  66,  ...,  49,  59,  64],\n",
              "           [ 68,  68,  68,  ...,  44,  55,  62]],\n",
              " \n",
              "          [[ 40,  40,  40,  ...,  30,  30,  30],\n",
              "           [ 40,  40,  40,  ...,  30,  30,  30],\n",
              "           [ 40,  40,  40,  ...,  28,  28,  28],\n",
              "           ...,\n",
              "           [ 37,  37,  37,  ...,  21,  27,  30],\n",
              "           [ 34,  34,  34,  ...,  12,  22,  27],\n",
              "           [ 36,  36,  36,  ...,   7,  18,  25]],\n",
              " \n",
              "          [[ 73,  73,  73,  ...,  46,  46,  46],\n",
              "           [ 73,  73,  73,  ...,  46,  46,  46],\n",
              "           [ 71,  71,  71,  ...,  44,  44,  44],\n",
              "           ...,\n",
              "           [ 58,  58,  58,  ...,  28,  34,  37],\n",
              "           [ 55,  55,  55,  ...,  19,  29,  34],\n",
              "           [ 57,  57,  57,  ...,  14,  25,  32]]],\n",
              " \n",
              " \n",
              "         [[[ 29,  30,  32,  ..., 115, 115, 115],\n",
              "           [ 30,  30,  31,  ..., 106, 106, 106],\n",
              "           [ 31,  30,  29,  ...,  99,  99,  99],\n",
              "           ...,\n",
              "           [ 25,  25,  25,  ..., 105, 154,  66],\n",
              "           [ 29,  29,  29,  ...,  54,  85,  49],\n",
              "           [ 31,  31,  31,  ...,  28,  49,  51]],\n",
              " \n",
              "          [[ 57,  58,  60,  ..., 131, 131, 131],\n",
              "           [ 58,  58,  59,  ..., 122, 122, 122],\n",
              "           [ 59,  58,  57,  ..., 115, 115, 115],\n",
              "           ...,\n",
              "           [ 57,  57,  57,  ..., 112, 161,  73],\n",
              "           [ 61,  61,  61,  ...,  61,  92,  56],\n",
              "           [ 63,  63,  63,  ...,  35,  56,  58]],\n",
              " \n",
              "          [[ 45,  46,  48,  ..., 105, 105, 105],\n",
              "           [ 46,  46,  47,  ...,  96,  96,  96],\n",
              "           [ 47,  46,  45,  ...,  89,  89,  89],\n",
              "           ...,\n",
              "           [ 42,  42,  42,  ..., 104, 153,  65],\n",
              "           [ 46,  46,  46,  ...,  53,  84,  48],\n",
              "           [ 48,  48,  48,  ...,  27,  48,  50]]]], dtype=torch.uint8),\n",
              " 'label': tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        dtype=torch.float64)}"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3333, 186, 186)"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_loader), len(validation_loader), len(test_loader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
